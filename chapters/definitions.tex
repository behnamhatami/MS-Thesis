
\فصل{مفاهیم اولیه}

در این فصل به تعریف و بیان مفاهیم پایه‌ای مورد استفاده در فصل‌های بعد می‌پردازیم.
با توجه به مطالب مورد نیاز در فصل‌های آتی، مطالب این فصل به سه بخش، مسائل ان‌پی-سخت، الگوریتم‌های تقریبی و الگوریتم‌های جویبار داده تقسیم می‌شود.

\قسمت{مسائل ان‌پی-سخت}

یکی از اولین سؤال‌های بنیادی مطرح در علم کامپیوتر، اثبات \مهم{عدم حل‌پذیری} بعضی از مسائل است.
به عنوان نمونه، می‌توان از دهمین مسئله‌ی هیلبرت\پاورقی{Hilbert} در کنگره‌ی ریاضی یاد کرد.
هیلبرت این مسئله را این‌گونه بیان کرد:‌ ''فرآیندی طراحی کنید که در تعداد متناهی گام بررسی کند که آیا یک چندجمله‌ای، ریشه‌ی صحیح\پاورقی{Integral root} دارد یا خیر ``. 
با مدل محاسباتی که به‌وسیله‌ی تورینگ\پاورقی{Touring} ارائه شد، این مسئله معادل پیدا کردن الگوریتمی برای این مسئله است که اثبات می‌شود امکان‌پذیر نیست ~\مرجع{sipser2012introduction}.
برخلاف مثال بالا، عمده‌ی مسائل علوم کامپیوتر از نوع بالا نیستند و برای طیف وسیعی از آن‌ها، الگوریتم‌های پایان‌پذیر وجود دارد.
بیش‌تر تمرکز علوم کامپیوتر هم بر روی چنین مسائلی است.

اگر چه برای اکثر مسائل، الگوریتمی پایان‌پذیر وجود دارد، اما وجود چنین الگوریتمی لزومی بر حل شدن چنین مسائلی نیست.
در عمل، علاوه بر وجود الگوریتم، میزان کارآمدی\پاورقی{Efficiency} الگوریتم نیز مطرح می‌گردد.
به طور مثال، اگر الگوریتم حل یک مسئله مرتبه‌ی بالا یا نمایی داشته باشد، الگوریتم ارائه‌شده برای آن مسئله برای ورودی‌های نسبتا بزرگ قابل اجرا نیست و نمی‌توان از آن برای حل مسئله استفاده کرد.
برای تشخیص و تمیز کارآمدی الگوریتم‌های مختلف و همچنین میزان سختی مسائل در امکان ارائه‌ی الگوریتم‌های کارآمد یا غیر کارآمد، نظریه‌ی پیچیدگی\پاورقی{Complexity theory}، دسته‌بندی‌های مختلفی برای سختی مسائل و حل‌پذیری آن‌ها ارائه داده است تا بتوان به‌طور رسمی\پاورقی{Formal} در مورد این معیارها صحبت کرد.
برای دسته‌بندی مسائل در نظریه‌ی پیچیدگی، ابتدا آن‌ها را به صورت تصمیم پذیر بیان می‌کنند.

\شروع{مسئله}

\مهم{(مسائل تصمیم‌گیری)}\پاورقی{Decision problems} به دسته‌ای از مسائل گفته می‌شود که پاسخ آن‌ها تنها بله یا خیر است.

\پایان{مسئله}

به عنوان مثال، اگر بخواهیم مسئله‌ی $1$-مرکز در فضای $\IR^d$ را به صورت تصمیم پذیر بیان کنیم، به مسئله‌ی زیر می‌رسیم:

\شروع{مسئله}

\مهم{(نسخه‌ی تصمیم پذیر $1$-مرکز)} مجموعه‌ی نقاط در فضا $\IR^d$ و شعاع $r$ داده شده است، آیا دایره‌ای به شعاع $r$ وجود دارد که تمام نقاط را بپوشاند؟

\پایان{مسئله}

در نظریه‌ی پیچیدگی، می‌توان گفت عمده‌ترین دسته‌بندی موجود، دسته‌بندی مسائل تصمیم‌گیری به مسائل پی $(P)$ و ان‌پی $(NP)$ است.
رده‌ی مسائل $P$، شامل تمامی مسائل تصمیم‌گیری است که راه‌حل چندجمله‌ای برای آن‌ها وجود دارد.
از طرفی رده‌ی مسائل $NP$، شامل تمامی مسائل تصمیم‌گیری است که در زمان چندجمله‌ای قابل صحت‌سنجی\پاورقی{Verifiable} اند.
تعریف صحت‌سنجی در نظریه پیچیدگی، یعنی اگر جواب مسئله‌ی تصمیم‌گیری بله باشد، می‌توان اطلاعات اضافی با طول چندجمله‌ای ارائه داد، که در زمان چندجمله‌ای از روی آن، می‌توان جواب بله الگوریتم را تصدیق نمود.
به طور مثال، برای مسئله‌ی $1$-مرکز، کافی است به عنوان تصدیق جواب بله، مرکز دایره‌ی پوشاننده، ارائه داده شود.
در این صورت، می‌توان با مرتبه‌ی خطی بررسی نمود که تمام نقاط داخل این دایره قرار می‌گیرند یا نه.
برای مطالعه‌ی بیش‌تر و تعاریف دقیق‌تر می‌توان به مرجع ~\مرجع{sipser2012introduction} مراجعه نمود.

همان‌طور که می‌دانید درستی یا عدم درستی $P \subset NP$ از جمله معروف‌ترین مسائل حل‌نشده\پاورقی{Open problem} در نظریه پیچیدگی است.
حدس بسیار قوی وجود دارد که $P \neq NP$ و بسیاری از مسائل، با این فرض حل می‌شوند و درصورتی‌که زمانی، خلاف این فرض اثبات گردد، آنگاه قسمت عمده‌ای از علوم کامپیوتر زیر سؤال می‌رود.

در نظریه‌ی پیچیدگی، برای دسته‌بندی مسائل، یکی از روش‌های دسته‌بندی کاهش چندجمله‌ای\پاورقی{Polynomial Reduction} مسائل به یکدیگر است. 

\شروع{تعریف}

می‌گوییم مسئله‌ی $A$ در زمان چندجمله‌ای به مسئله‌ی $B$ کاهش می‌یابد، اگر وجود داشته باشد الگوریتم چندجمله‌ای $C$ که به ازای هر ورودی $\alpha$ برای مسئله‌ی $A$، یک ورودی $\beta$ در زمان چندجمله‌ای برای مسئله‌ی $B$ بسازد، به‌طوری‌که $A$، $\alpha$ را می‌پذیرد اگر و تنها اگر $B$، $\beta$ را بپذیرد. در اینجا منظور از پذیرفتن جواب بله به ورودی است.

\پایان{تعریف}

از این به بعد برای سادگی به جای \مهم{کاهش چندجمله‌ای} از واژه‌ی \مهم{کاهش} استفاده می‌کنیم.
در پی جستجوهایی که برای برابری دسته‌ی پی و ان‌پی صورت گرفت، مجموعه‌ای از مسائل که عمدتاً داخل ان‌پی هستند استخراج گردید که اگر ثابت شود یکی از آن‌ها متعلق به پی است، آنگاه تمام مسائل دسته‌ی ان‌پی متعلق به پی خواهند بود و در نتیجه $P = NP$ می‌گردد.
به این مجموعه مسائل ان‌پی-سخت می‌گویند.
در واقع مسائل این دسته، مسائلی هستند که تمام مسائل داخل دسته‌ی ان‌پی، به آن‌ها کاهش می‌یابند.

کوک و لوین در قضیه‌ای به نام \مهم{قضیه‌ی کوک-لوین} ثابت کردند مسئله‌ی صدق‌پذیری\پاورقی{Satisfiability problem} یک مسئله‌ی ان‌پی-سخت است ~\مرجع{sipser2012introduction}.
با پایه قرار دادن این اثبات و استفاده از تکنیک کاهش، اثبات ان‌پی-سخت بودن سایر مسائل، بسیار ساده‌تر گردید.
در ادامه مسئله‌ی پوشش رأسی\پاورقی{Vertex Coverage} را تعریف می‌کنیم.

\زیرقسمت{پوشش رأسی}
در این پایان‌نامه، از این مسئله به عنوان مسئله‌ی پایه برای اثبات ان‌پی-سخت بودن مسئله‌ی $k$-مرکز استفاده می‌شود. تعریف این مسئله مطابق زیر است:

\شروع{مسئله}

\مهم{(پوشش رأسی)}گراف بدون جهت $G(V, E)$ داده شده است. هدف مسئله پیدا کردن مجموعه‌ی $S \subset V$ با کم‌ترین تعداد اعضا است به‌طوری‌که هر رأس $v \in V$ در یکی از شرایط زیر صدق کند:

\شروع{فقرات}

\فقره{$v \in S$}

\فقره{وجود دارد رأسی $u \in S$ به طوری $(v, u) \in E$}

\پایان{فقرات}

به عبارت ساده‌تر هر رأسی یا خودش یا یکی از همسایگانش داخل مجموعه‌ی $S$ قرار داشته باشد.
\پایان{مسئله}

نسخه‌ی تصمیم‌گیری این مسئله به این‌گونه تعریف می‌شود که آیا گراف داده‌شده دارای پوشش رأسی با اندازه‌ی $k$ است یا نه.

\شروع{قضیه}

مسئله‌ی پوشش رأسی، یک مسئله‌ی ان‌پی-سخت است.

\پایان{قضیه}

\شروع{اثبات}

برای مشاهده‌ی اثبات ان‌پی-سخت بودن مسئله‌ی پوشش رأسی, نیاز به زنجیره‌ای از مسائل که از مسئله‌ی صدق‌پذیری شروع می‌شود است, به‌طوری‌که هر عضو از این زنجیره، به عضو بعدی در زمان چندجمله‌ای کاهش می‌یابد و در نهایت نتیجه می‌شود که مسئله‌ی صدق‌پذیری در زمان چندجمله‌ای به مسئله‌ی پوشش رأسی کاهش می‌یابد و در نتیجه چون مسئله‌ی صدق‌پذیری یک مسئله‌ی ان‌پی-سخت است.
بنابراین مسئله‌ی پوشش رأسی نیز ان‌پی-سخت خواهد بود. برای مطالعه‌ی روند اثبات به مرجع ~\مرجع{sipser2012introduction} مراجعه کنید. 

\پایان{اثبات}

\قسمت{الگوریتم‌های تقریبی}

تا اینجا با رده‌بندی مسائل به دو دسته‌ی پی و ان‌پی آشنا شدیم.
نه تنها مسائل ان‌پی، بلکه بعضی از مسائل پی نیز دارای الگوریتم کارآمدی نیستند.
در عمل، یک الگوریتم چندجمله‌ای با مرتبه‌ی بالا (بیش از $4$)، یک الگوریتم کارامد محسوب نمی‌شود، زیرا نمی‌توان با کامپیوتر‌های عادی امروزی، برای $n$های بزرگ‌تر از $1000$ در زمانی کم‌تر از $1$ دقیقه به پاسخ رسید.
با این تعریف، به‌طور مثال هنوز الگوریتم کارآمدی برای فهمیدن این‌که یک عدد اول است یا نه پیدا نشده است، با اینکه الگوریتم ارائه شده یک الگوریتم چندجمله‌ای است.
عمده‌ی مسائل کاربردی مطرح در دنیای واقع، یا متعلق به دسته‌ی ان‌پی-سخت هستند و در نتیجه راه‌حل چندجمله‌ای ندارند، یا اگر راه‌حل چند جمله‌ای داشته باشند، مرتبه‌ی چندجمله‌ای ارائه شده بالاست و در نتیجه راه‌حل کارآمدی محسوب نمی‌گردد.
یکی از رویکردهای رایج در برابر چنین مسائلی، صرف نظر کردن از دقت راه‌حل‌هاست.
به‌طور مثال راه‌حل‌های ابتکاری\پاورقی{Heuristic} گوناگونی برای مسائل مختلف به خصوص مسائل ان‌پی-سخت بیان شده است.
این راه‌حل‌ها بدون این‌که تضمین کنند راه‌حل خوبی ارائه می‌دهند یا حتی جوابشان به جواب بهینه نزدیک است، اما با معیارهایی سعی در بهینه عمل کردن دارند و تا حد ممکن سعی در ارائه‌ی جواب بهینه یا نزدیک بهینه دارند.
اما در عمل، تنها برای دسته‌ای از کاربردها پاسخ قابل قبولی ارائه می‌دهند. 

مشکل عمده‌ی راه‌حل‌های ابتکاری، عدم امکان استفاده از آن‌ها برای تمام کاربردها است.
بنابراین در رویکرد دوم که اخیراً نیز مطرح شده است، سعی در ارائه‌ی الگوریتم‌های ابتکاری شده است که تضمین می‌کنند اختلاف زیادی با الگوریتمی که جواب بهینه می‌دهد، نداشته باشند.
در واقع این الگوریتم‌ها همواره و در هر شرایطی، تقریبی از جواب بهینه را ارائه می‌دهند.
به چنین الگوریتم‌هایی، \مهم{الگوریتم‌های تقریبی}\پاورقی{Approximation Algorithm} می‌گویند.
علت اصلی این نام‌گذاری، تقریب زدن جواب الگوریتم بهینه است.
ضریب تقریب یک الگوریتم تقریبی، به حداکثر نسبت جواب الگوریتم تقریبی به جواب بهینه گفته می‌شود.

الگوریتم‌های تقریبی تنها به علت محدودیت کارایی الگوریتم‌هایی که جواب بهینه می‌دهند، مورد استفاده قرار نمی‌گیرند.
هر نوع محدودیتی ممکن است، استفاده از الگوریتم تقریبی را نسبت به الگوریتمی که جواب بهینه می‌دهد، مقرون به صرفه کند.
به طور مثال از جمله عوامل دیگری که ممکن است باعث این انتخاب شود، کاهش میزان حافظه‌ی مصرفی باشد.
برای طیف وسیعی از مسائل، کمبود حافظه، باعث می‌شود الگوریتم‌هایی با حافظه‌ی مصرفی کم‌تر طراحی شود که به دقت الگوریتم‌های بهینه عمل نمی‌کند اما می‌تواند با مصرف کم‌تر به تقریبی از جواب بهینه دست یابد.
معمولاً چنین الگوریتم‌هایی حافظه‌ی مصرفی از مرتبه‌ی زیرخطی\پاورقی{Sublinear} دارند و به همین دلیل برای داده‌های حجیم بسیار کاربرد دارند.

\شروع{لوح}[t]
\وسط‌چین
\شروع{جدول}{|c|c|}
\خط‌پر
مسئله & کران پایین تقریب‌پذیری
\\
\خط‌پر
\خط‌پر
پوشش رأسی &$1.3606$ \مرجع{dinur2005hardness} \\
$k$-مرکز & $2$\مرجع{vazirani2013approximation} \\ 
$k$-مرکز در فضای اقلیدسی & $1.822$\مرجع{bern1996approximation} \\
$1$-مرکز در حالت جویبار داده & $\frac{1 + \sqrt{2}}{2}$ \مرجع{agarwal2010streaming} \\
$k$-مرکز با نقاط پرت و نقاط اجباری & $3$\مرجع{charikar2001algorithms}\\
\خط‌پر
\پایان{جدول}

\شرح{نمونه‌هایی از ضرایب تقریب برای مسائل بهینه‌سازی}
\برچسب{جدول:تقریب‌پذیری}
\پایان{لوح}

\زیرقسمت{میزان تقریب‌پذیری مسائل}

همان‌طور که تا اینجا دیدیم، یکی از راه‌کارهایی که برای کارآمد کردن راه‌حل ارائه شده برای یک مسئله وجود دارد، استفاده از الگوریتم‌های تقریبی برای حل آن مسئله است.
یکی از عمده‌ترین دغدغه‌های مطرح در الگوریتم‌های تقریبی کاهش ضریب تقریب است.
در بعضی از موارد حتی امکان ارائه‌ی الگوریتم تقریبی با ضریبی ثابت نیز وجود ندارد.
به طور مثال، همان‌طور که در فصل کارهای پیشین بیان خواهد شد، الگوریتم تقریبی با ضریب تقریب کم‌تر از $2$، برای مسئله‌ی $k$-مرکز وجود ندارد مگر اینکه $P = NP$ باشد.
برای مسائل مختلف، معمولاً می‌توان کران پایینی برای میزان تقریب‌پذیری آن‌ها ارائه داد.
در واقع برای مسائل ان‌پی-سخت، علاوه بر این الگوریتم کارآمدی وجود ندارد، بعضاً الگوریتم تقریبی برای حل آن با ضریبی تقریب کم و نزدیک به یک نیز وجود ندارد.
در جدول \رجوع{جدول:تقریب‌پذیری} از میزان تقریبی مسائل مختلفی که در این پایان‌نامه مورد استفاده قرار می‌گیرد ببینید.

\قسمت{الگوریتم‌های جویبارداده}

در علوم کامپیوتر، کاربردهایی وجود دارد که تمام داده‌ی ورودی در لحظه‌ی شروع الگوریتم در دسترس نیستند.
بنابراین مدل جدیدی برای این الگوریتم‌ها ارائه می‌شود که در آن ورودی به مرور زمان در اختیار الگوریتم قرار می‌گیرد.
به علت حجم زیاد داده‌ها، معمولاً امکان ذخیره‌سازی داده‌ها برای استفاده‌های بعدی وجود ندارد و در نتیجه، در این الگوریتم‌ها تنها می‌توان چند بار (معمولاً یک بار) از ابتدای ورودی تا انتهای ورودی، به داده‌ها دسترسی داشت.
در واقع الگوریتم‌های جویبار داده، به الگوریتم‌هایی گفته می‌شوند که ورودی آن‌ها یک یا چند دنباله است که الگوریتم می‌تواند به ترتیب دنباله، یک یا چند بار از ابتدای دنباله تا انتهای آن، به اعضای دنباله دسترسی داشته باشد.

الگوریتم‌های جویبار داده معمولاً محدودیت شدیدی در میزان حافظه دارند (نسبت به اندازه‌ی ورودی) و به علت تعداد زیاد داده‌ها، محدودیت زمانی پردازش برای هر داده نیز مطرح است. چنین محدودیت‌هایی معمولاً باعث می‌شود که الگوریتم جویبار داده تنها بتواند یک جواب تقریبی از جواب بهینه را با استفاده از اطلاعات مختصری که در حافظه نگه می‌دارد ارائه دهد.

در سال‌های اخیر، پیشرفت‌های حوزه‌ی تکنولوژی، امکان جمع‌آوری داده‌ها را به صورت پیوسته ممکن ساخته است.
به طور مثال می‌توان تراکنش‌های بانکی، استفاده از تلفن همراه یا مرورگر وب خود را در نظر بگیرید.
در هر تعاملی که با این سیستم‌ها انجام می‌دهید، میزان زیادی داده تولید شده و ذخیره می‌گردد.
در اکثر مواقع می‌توان این حجم عظیم داده را مورد پردازش قرار داد و اطلاعات بسیار مفیدی از آن، استخراج نمود.
زمانی که حجم داده بسیار زیاد باشد، معمولاً چالش‌های محاسباتی و الگوریتمی برای الگوریتم‌ها به وجود می‌آورد.
از جمله‌ی آن‌ها می‌توان به موارد زیر اشاره کرد:

\شروع{فقرات}
 
\فقره{با افزایش حجم داده، امکان پردازش داده‌ها به صورت کارآمد با چند بار عبور کردن از جویبار داده وجود ندارد.
یکی از مهم‌ترین موانع در طراحی الگوریتم‌های کارآمد برای مدل جویبار داده این است که الگوریتم‌ها مجبور هستند هر داده را حداکثر یک بار مورد پردازش قرار دهند.
بنابراین الگوریتم‌های مدل جویبار داده، معمولاً تک‌گذره‌اند.}

\فقره{در اکثر الگوریتم‌های جویبار داده، از یک واحد برای پردازش داده‌ها به صورت محلی استفاده می‌شود.
علت اصلی وجود چنین واحدی، نیاز این الگوریتم‌ها به تشخیص تغییرات در داده‌های ورودی است.
این عملکرد الگوریتم‌های جویبار داده را می‌توان نوعی استفاده از اصل محل‌گرایی\پاورقی{Locality} به حساب آورد.
اما مشکل عمده‌ی این نوع رویکرد اجتناب‌ناپذیر، در عمده‌ی موارد، عدم امکان ارائه‌ی راه‌حلی مناسب برای مسئله است.
در ساخت الگوریتم‌های جویبار داده، باید دقت و تمرکز عمده‌ای را صرف تشخیص نحوه‌ی تغییر داده‌های ورودی نمود. }

\فقره{یکی از مهم‌ترین مشخصه‌های جویبار داده‌ها، ماهیت عدم متمرکز بودن داده‌ها است.
از طرفی یک پردازنده به تنهایی دارای محدودیت بسیار زیادی از نظر قدرت پردازشی و حافظه است.
بنابراین معمولاً الگوریتم‌های جویبار داده، به‌گونه‌ای طراحی می‌شوند که توزیع‌پذیر بوده و توانایی اجرا شدن به صورت چندروندی\پاورقی{Multi Thread} را دارا باشند.}

\پایان{فقرات} 

\زیرقسمت{گونه‌های مطرح}
در مدل جویبار داده، بعضی یا همه‌ی نقاط ورودی که باید پردازش گردند برای دسترسی تصادفی\پاورقی{Random access} از حافظه‌ی اصلی یا حافظه‌ی جانبی در دسترس نیستند، بلکه به مرور زمان، به صورت جویبار یا جویبارهایی از داده در اختیار الگوریتم قرار می‌گیرند \مرجع{aggarwal2007data}.

هر جویبار را می‌توان به عنوان دنباله‌ای مرتب از نقاط (یا به‌روزرسانی‌ها\پاورقی{Update}) در نظر گرفت به طوری که به ترتیب دنباله قابل دسترسی هستند و هر کدام را تنها به تعدادی محدود بار (معمولاً یک بار) می‌توان خواند \مرجع{aggarwal2007data}.

مسائل زیادی در این مدل مورد بررسی قرار گرفته‌اند که از جمله مهم‌ترین آن‌ها می‌توان به موارد زیر اشاره کرد:

\شروع{فقرات}

\فقره{محاسبه‌ی آماری مربوط به توزیع داده‌های یک جویبار داده به قدری بزرگ که قابل ذخیره شدن نیست.}

\فقره{مسائل مربوط به گراف‌ها، به‌طوری‌که ماتریس مجاورت گراف به ترتیب دلخواهی به صورت جویبار داده به الگوریتم داده می‌شود.}

\فقره{مسائل مربوط به دنباله‌ها و استخراج اطلاعات از دنباله‌ها که وابستگی شدیدی به ترتیب اعضای دنباله دارند، مانند پیدا کردن طولانی‌ترین زیردنباله با اعضای صعودی یا پیدا کردن تعداد نابه‌جایی‌های\پاورقی{Inversion} داخل دنباله. \مرجع{aggarwal2007data} }

\زیرقسمت{تحلیل الگوریتم‌های جویبار داده}

کارایی یک الگوریتم جویبار داده بر اساس سه معیار زیر اندازه‌گیری می‌شود:

\شروع{فقرات}

\فقره{تعداد مرتبه‌ای که الگوریتم از روی جویبار داده گذر می‌کند}

\فقره{میزان حافظه‌ی مصرفی}

\فقره{زمان مصرفی به ازای پردازش هر داده}

\پایان{فقرات}

الگوریتم‌های جویبار داده تشابه‌های زیادی با الگوریتم‌های برخط\پاورقی{Online} دارند. به طور مثال، هر دو نوع الگوریتم، نیاز دارند قبل از اینکه تمام ورودی فرا برسد، در مورد داده‌ی جویبار داده تصمیم بگیرند.
اما تفاوت‌های عمده‌ای نیز بین این الگوریتم‌ها وجود دارد.
الگوریتم‌های جویبار داده، دارای حافظه‌ی محدودی هستند، اما می‌توانند تصمیم‌گیری راجع به یک داده را تا چند مرحله به عقب بیاندازند، در صورتی که الگوریتم‌های برخط، به محض ورود داده، باید در مورد آن تصمیم بگیرند.

\پایان{فقرات}

\زیرقسمت{مجموعه هسته}

یکی از تکنیک‌های رایج در الگوریتم‌های جویبار داده، نگه‌داری نماینده‌ای با اندازه‌ی بسیار کوچک‌تر نسبت به‌اندازه‌ی جویبار داده است.
این مجموعه معمولاً دغدغه‌ی محدودیت حافظه را در الگوریتم‌های جویبار داده برطرف می‌کند. به چنین مجموعه‌ای, مجموعه هسته می‌گوییم.
حال به تعریف رسمی مجموعه هسته می‌پردازیم:

\شروع{تعریف}

فرض کنید $\mu$ یک تابع اندازه‌گیری\پاورقی{Measure function}(همانند تابع عرض مجموعه‌ای از نقاط) از $\IR^d$ به اعداد حقیقی نامنفی $\IR^+ \cup \set{0}$ باشد.
فرض کنید که این تابع، یک تابع یکنواخت است، یعنی به ازای هر دو مجموعه‌ی $P_1$ و $P_2$ که $P_1 \subset P_2$ است، آنگاه
$$\mu(P_1) \leq \mu(P_2)$$

فرض کنید $\epsilon > 0$ داده شده است، به زیرمجموعه‌ی $Q \subseteq P$ یک $\epsilon$-مجموعه‌ی هسته برای مجموعه $P$ گویند اگر رابطه‌ی زیر برقرار باشد:
$$(1 - \epsilon) \mu(P) \leq \mu (Q)$$

\پایان{تعریف}

یکی از مجموعه هسته‌های معروف, مجموعه هسته‌ی مطرح برای تابع اندازه‌گیری عرض نقاط است. به چنین مجموعه هسته‌ای به اختصار $\epsilon$-هسته\پاورقی{$\epsilon$-Kernel} گفته می‌شود. تعریف دقیق $\epsilon$-هسته در زیر آمده است:

\شروع{تعریف}

فرض کنید $S^{d-1}$ کره‌ی واحد با مرکز واقع در مبدأ در فضای $\IR^d$ باشد.
به ازای هر مجموعه $P$ از‌ نقاط در فضای $\IR^d$ و هر جهت دلخواه $u \in S^{d-1}$، عرض جهت‌دار $P$ در جهت $u$ که با نماد $\omega(u, P)$ تعریف می‌شود، مطابق زیر است:
$$\omega(u, P) = \max_{p \in P} \langle u, p\rangle - \min_{p \in P} \langle u, p \rangle$$
که در آن $\langle ., . \rangle$ همان ضرب داخلی دو بردار است.
برای متغیر $\epsilon > 0$، زیرمجموعه‌ی $Q \subset P$ یک $\epsilon$-هسته نامیده می‌شود اگر به ازای هر $u \in S^{d-1}$ داشته باشیم:
$$(1 - \epsilon) \omega(u, P) \leq \omega(u, Q)$$

\پایان{تعریف}

$\epsilon$-هسته 
یکی از اساسی‌ترین مجموعه هسته‌های مطرح است و برای طیف وسیعی از مسائل قابل استفاده است.
الگوریتم‌های زیادی برای محاسبه‌ی $\epsilon$-هسته در حالت ایستا ارائه شده است \مرجع{agarwal2004approximating}. 

یکی از روش‌هایی که در تبدیل یک الگوریتم از حالت ایستا به حالت جویبار داده ارائه شده است، استفاده از روش دوبرابرسازی\پاورقی{Doubling} است.
این روش یک روش عام و قابل استفاده برای طیف وسیعی از مسائل است.
به عنوان نمونه، دوبرابرسازی را بر روی $\epsilon$-هسته مورد بررسی قرار می‌دهیم. $\epsilon$-هسته، دارای دو خاصیت اساسی زیر است که برای تکنیک دوبرابرسازی قابل استفاده می‌شود.


\شروع{فقرات}

\فقره{اگر $P_2$ یک $\epsilon$-هسته برای $P_1$ باشد و $P_3$ یک $\delta$-هسته برای $P_2$ باشد، آنگاه $P_3$ یک $(\epsilon + \delta)$-هسته برای $P_1$ است.}

\فقره{اگر $Q_1$ یک $\epsilon$-هسته برای $P_1$ باشد و $Q_2$ یک $\epsilon$-هسته برای $P_2$ باشد، آنگاه $Q_1 \cup Q_2$ یک $\epsilon$-هسته برای $P_1 \cup P_2$ است.}

\پایان{فقرات}

ایده‌ی اصلی تکنیک دوبرابرسازی، استفاده از روش مبنای دو است، که در ابتدا از نقطه‌ی اول یک مجموعه هسته ساخته می‌شود.
سپس با نقطه‌ی بعدی یک مجموعه هسته از دو نقطه‌ی فعلی ساخته می‌شود.
سپس با دو نقطه‌ی بعدی یک مجموعه‌ی هسته از چهار نقطه فعلی ساخته می‌شود.
اگر این روند را به صورت توان‌هایی از ۲ ادامه بدهیم، در هر لحظه حداکثر، $\log(n)$ مجموعه هسته داریم که در طول الگوریتم، بعضی از آن‌ها با یکدیگر ادغام می‌گردند و پس از ادغام، یک هسته از کل آن‌ها به عنوان نماینده انتخاب می‌گردد و مجموعه هسته‌ی جدیدی از روی آن ساخته می‌شود.
در واقع اگر دوباره بر روی دو مجموعه‌ی ادغام شده هسته حساب نشود، اندازه‌ی هسته به صورت نمایی افزایش می‌یابد که مطلوب نیست.
از این رو برای کاهش حافظه‌ی مصرفی از روی دو هسته‌ی ادغام شده، یک هسته‌ی جدید محاسبه می‌گردد.
این کار، ضریب تقریب را افزایش می‌دهد ولی باعث می‌شود، حافظه‌ی مصرفی، حداکثر $\log(n)$ برابر حافظه‌ی مصرفی در حالت ایستا می‌شود.